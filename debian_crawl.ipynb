{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Issues\n",
    "- jessie release only found in https://qa.debian.org/madison.php?package=cmake, not found in http://ftp.debian.org/debian/pool/main/c/cmake/ \n",
    "- slow speed of getting sources? is there a faster way?\n",
    "- how to deal with new versions without date? (so far only 1624 debian library versions without release date in scantist_library_version)\n",
    "- link to data-staging-test instead of sqlite for better monitoring, and data persistence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DB functions\n",
    "import os\n",
    "import psycopg2\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "load_dotenv('../pipeline_monitoring/.env')\n",
    "\n",
    "def sqlite_create_conn(db_filename):\n",
    "    '''\n",
    "    Create connection to temp sqlite database\n",
    "    '''\n",
    "    conn = sqlite3.connect(db_filename)\n",
    "    return conn\n",
    "\n",
    "def sqlite_init_tables(conn):\n",
    "    '''\n",
    "    Create scantist_library and scantist_library_version tables in sqlite\n",
    "    '''\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    cursor.execute('''CREATE TABLE IF NOT EXISTS scantist_library (\n",
    "        id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "        created timestamptz NOT NULL,\n",
    "        modified timestamptz NOT NULL,\n",
    "        \"name\" varchar(255) NULL,\n",
    "        description text NULL,\n",
    "        vendor varchar(255) NULL,\n",
    "        \"language\" varchar(64) NULL,\n",
    "        platform varchar(255) NULL,\n",
    "        \"source\" varchar(255) NULL,\n",
    "        processed_time timestamptz NULL,\n",
    "        is_valid bool NULL,\n",
    "        CONSTRAINT scantist_library_name_vendor_platform_source_db9dacce_uniq UNIQUE (name, vendor, platform)\n",
    "    );''')\n",
    "\n",
    "    cursor.execute('''CREATE TABLE IF NOT EXISTS scantist_library_version (\n",
    "        id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "        created timestamptz NOT NULL,\n",
    "        modified timestamptz NOT NULL,\n",
    "        version_number varchar(255) NULL,\n",
    "        release_date timestamptz NULL,\n",
    "        is_active bool NOT NULL,\n",
    "        is_officially_supported bool NOT NULL,\n",
    "        library_id int4 NULL,\n",
    "        license_id int4 NULL,\n",
    "        is_clean bool NOT NULL,\n",
    "        \"source\" varchar(16) NULL,\n",
    "        processed_time timestamptz NULL,\n",
    "        is_valid bool NULL,\n",
    "        CONSTRAINT scantist_library_version_unique_key UNIQUE (library_id, version_number)\n",
    "    );''')\n",
    "\n",
    "\n",
    "    conn.commit()\n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "\n",
    "\n",
    "def cve_create_conn():\n",
    "    '''\n",
    "    Creation connection to CVETRIAGE database\n",
    "    '''\n",
    "    # Replace these with your own database credentials\n",
    "    host2 = os.environ.get('CVETRIAGE_POSTGRES_HOSTNAME')\n",
    "    database2 = os.environ.get('CVETRIAGE_POSTGRES_DB')\n",
    "    user2 = os.environ.get('CVETRIAGE_POSTGRES_USERNAME')\n",
    "    password2 = os.environ.get('CVETRIAGE_POSTGRES_PASSWORD')\n",
    "\n",
    "    # Establish the connection\n",
    "    connection_cve = psycopg2.connect(\n",
    "        host=host2,\n",
    "        database=database2,\n",
    "        user=user2,\n",
    "        password=password2\n",
    "    )\n",
    "    return connection_cve\n",
    "\n",
    "def querysqlite_insert_library_multiple(conn,debian_binaries_to_crawl,ftp_dict):\n",
    "    '''\n",
    "    Upsert into scantist_library\n",
    "    debian_binaries_to_crawl:\n",
    "    ftp_dict:\n",
    "    '''\n",
    "    cursor = conn.cursor()\n",
    "    debian_binary_tup=[]\n",
    "    curr_time=datetime.now().strftime(\"%Y-%m-%d\")\n",
    "    for binary_name in debian_binaries_to_crawl.keys():\n",
    "        # for v in debian_binaries_to_crawl[binary_name]:\n",
    "        #     if v in ftp_dict[binary_name].keys():\n",
    "        debian_binary_tup.append((curr_time,curr_time,binary_name,'','','','Debian','Scantist',None,True))\n",
    "    columns_library=', '.join(['created','modified','name','description','vendor','language','platform','source','processed_time','is_valid'])\n",
    "    sql_query_insert_library = f\"INSERT OR IGNORE INTO scantist_library ({columns_library}) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\"\n",
    "    cursor.executemany(sql_query_insert_library, debian_binary_tup)\n",
    "    # Commit the changes to save them to the database file\n",
    "    conn.commit()\n",
    "    cursor.close()\n",
    "\n",
    "def querysqlite_insert_library_versions_ea_binary(conn,binary_name,debian_binaries_to_crawl,ftp_dict):\n",
    "    '''\n",
    "\n",
    "    '''\n",
    "    cursor = conn.cursor()\n",
    "    curr_time=datetime.now().strftime(\"%Y-%m-%d\")\n",
    "    sql_query_select_library_id=f\"select id from scantist_library where name='{binary_name}'\"\n",
    "    columns_library_version=', '.join(['created','modified','version_number','release_date','is_active','is_officially_supported','library_id','license_id','is_clean','source','processed_time','is_valid'])\n",
    "    sql_query_insert_library_version = f\"INSERT OR IGNORE INTO scantist_library_version ({columns_library_version}) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\"\n",
    "    \n",
    "    # get library_id of binary\n",
    "    cursor.execute(sql_query_select_library_id)\n",
    "    result_id = cursor.fetchall()[0][0]\n",
    "    print(f'Library_id for {binary_name} is {result_id}')\n",
    "\n",
    "    debian_binary_v_tup=[]\n",
    "    for v in debian_binaries_to_crawl[binary_name]:\n",
    "        if v in ftp_dict[binary_name].keys():\n",
    "            debian_binary_v_tup.append((curr_time,curr_time,v,min(ftp_dict[binary_name][v]),True,True,result_id,None,True,'Scantist',None,True))\n",
    "\n",
    "    cursor.executemany(sql_query_insert_library_version, debian_binary_v_tup)\n",
    "    # Commit the changes to save them to the database file\n",
    "    conn.commit()\n",
    "    cursor.close()\n",
    "\n",
    "def querysqlite_insert_library_versions_ea_binary2(conn,binary_name,version_date_list):\n",
    "    '''\n",
    "\n",
    "    '''\n",
    "    cursor = conn.cursor()\n",
    "    curr_time=datetime.now().strftime(\"%Y-%m-%d\")\n",
    "    sql_query_select_library_id=f\"select id from scantist_library where name='{binary_name}'\"\n",
    "    columns_library_version=', '.join(['created','modified','version_number','release_date','is_active','is_officially_supported','library_id','license_id','is_clean','source','processed_time','is_valid'])\n",
    "    sql_query_insert_library_version = f\"INSERT OR IGNORE INTO scantist_library_version ({columns_library_version}) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\"\n",
    "    \n",
    "    # get library_id of binary\n",
    "    cursor.execute(sql_query_select_library_id)\n",
    "    result_id = cursor.fetchall()[0][0]\n",
    "    print(f'Library_id for {binary_name} is {result_id}')\n",
    "\n",
    "    debian_binary_v_tup=[]\n",
    "    for v_date in version_date_list:\n",
    "        debian_binary_v_tup.append((curr_time,curr_time,v_date[0],v_date[1],True,True,result_id,None,True,'Scantist',None,True))\n",
    "\n",
    "    cursor.executemany(sql_query_insert_library_version, debian_binary_v_tup)\n",
    "    # Commit the changes to save them to the database file\n",
    "    conn.commit()\n",
    "    cursor.close()\n",
    "\n",
    "def udd_create_conn():\n",
    "    '''\n",
    "    Create connection to UDD database\n",
    "    '''\n",
    "    host1 = \"udd-mirror.debian.net\"\n",
    "    database1 = \"udd\"\n",
    "    user1 = \"udd-mirror\"\n",
    "    password1 = \"udd-mirror\"\n",
    "\n",
    "    # Establish the connection\n",
    "    connection_udd = psycopg2.connect(\n",
    "        host=host1,\n",
    "        database=database1,\n",
    "        user=user1,\n",
    "        password=password1\n",
    "    )\n",
    "    return connection_udd\n",
    "\n",
    "def querycve_all_debian_binary(connection_cve):\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    cursor = connection_cve.cursor()\n",
    "    query_debian_select_binaries = f'''SELECT name, description\n",
    "    from scantist_library\n",
    "    where platform='Debian'\n",
    "    and is_valid=True\n",
    "    '''\n",
    "    cursor.execute(query_debian_select_binaries)\n",
    "    result = cursor.fetchall()\n",
    "    # df_debian_binaries = pd.read_sql_query(query_debian_select_binaries, connection_cve)\n",
    "    # debian_binary_list = df_debian_binaries.name.to_list()\n",
    "    return result\n",
    "\n",
    "def querycve_binary_version_date_mapping(connection_cve,debian_binary_list):\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    binary_version_mapping = {}\n",
    "    cursor = connection_cve.cursor()\n",
    "    for binary_name in debian_binary_list:\n",
    "        # Execute your SQL query\n",
    "        query_binary_versions = f'''SELECT slv.version_number, slv.release_date from scantist_library sl\n",
    "        join scantist_library_version slv on slv.library_id=sl.id\n",
    "        where sl.name='{binary_name[0]}'\n",
    "        and sl.platform='Debian'\n",
    "        and slv.is_valid=True\n",
    "        order by slv.release_date desc\n",
    "        '''\n",
    "        cursor.execute(query_binary_versions)\n",
    "\n",
    "        # Fetch all the rows as a list of tuples\n",
    "        result = cursor.fetchall()\n",
    "        binary_version_mapping[binary_name] = result\n",
    "    cursor.close()\n",
    "    return binary_version_mapping\n",
    "\n",
    "def queryudd_cvetriage_missing_binary_versions(connection_udd,binary_version_mapping):\n",
    "    '''\n",
    "\n",
    "    '''\n",
    "    cursor = connection_udd.cursor()\n",
    "    debian_binaries_to_crawl = {}\n",
    "    for binary_info in binary_version_mapping.keys():\n",
    "        # Execute your SQL query\n",
    "        binary_name=binary_info[0]\n",
    "        query_latest_binary_versions = f\"SELECT distinct version FROM public.packages where package='{binary_name}'\"\n",
    "        cursor.execute(query_latest_binary_versions)\n",
    "        # Fetch all the rows as a list of tuples\n",
    "        udd_latest_versions_tup_list = cursor.fetchall()\n",
    "        udd_latest_versions_set = set([tup[0] for tup in udd_latest_versions_tup_list])\n",
    "        cve_versions_set = set(tup[0] for tup in binary_version_mapping[binary_info])\n",
    "        missing_latest_versions_set = udd_latest_versions_set - cve_versions_set\n",
    "        debian_binaries_to_crawl[binary_name]=missing_latest_versions_set\n",
    "    cursor.close()\n",
    "    return debian_binaries_to_crawl\n",
    "\n",
    "def queryudd_source_all_version_date_df(connection_udd,source_name):\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    query = f\"SELECT version, date FROM upload_history where source='{source_name}'\"\n",
    "    df = pd.read_sql_query(query, connection_udd)\n",
    "    return df\n",
    "\n",
    "## webscrape\n",
    "def web_parse_ftp_binary_pool_latest_versions(url):\n",
    "    '''\n",
    "    For a given source (based on url provided), \n",
    "    get the respective binaries latest version info (binary file_name: <binary-name>_<version>_<architecture>.deb and last_modified)\n",
    "    '''\n",
    "    file_info_list = []\n",
    "    # Send an HTTP GET request to the URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Check if the request was successful (status code 200)\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content of the webpage using BeautifulSoup\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Find all <tr> elements on the webpage\n",
    "        tr_elements = soup.find_all('tr')\n",
    "\n",
    "        # Extract the names and last modified timestamps from each <tr> element\n",
    "        \n",
    "        for tr in tr_elements:\n",
    "            td_elements = tr.find_all('td')\n",
    "            if len(td_elements) >= 4:\n",
    "                name = td_elements[1].text.strip()\n",
    "                last_modified = td_elements[2].text.strip()\n",
    "                file_info_list.append((name, last_modified))\n",
    "        file_info_list.remove(('Parent Directory', ''))\n",
    "    else:\n",
    "        print(f\"Failed to retrieve the webpage. Status code: {response.status_code}\")\n",
    "    \n",
    "    return file_info_list\n",
    "\n",
    "def web_parse_ftp_binary_pool_latest_versions_map(ftp_list):\n",
    "    '''\n",
    "    From the ftp_list (binary filename & last_modified),\n",
    "    group by binary_name, version, and last_modified\n",
    "    '''\n",
    "    ftp_dict={}\n",
    "    for _ in ftp_list:\n",
    "        if _[0].endswith('.deb'): # filter out non deb files\n",
    "            binary_name=_[0].split('_')[0]\n",
    "            binary_version=_[0].split('_')[1]\n",
    "            \n",
    "            release_date=_[1].split(' ')[0]\n",
    "            if binary_name not in ftp_dict:\n",
    "                ftp_dict[binary_name]={}\n",
    "            if binary_version in ftp_dict[binary_name]:\n",
    "                ftp_dict[binary_name][binary_version].update([release_date])\n",
    "            else:\n",
    "                ftp_dict[binary_name][binary_version]=set([release_date])\n",
    "    return ftp_dict\n",
    "\n",
    "def web_binary_all_versions_df(binary_name):\n",
    "    url = f'https://snapshot.debian.org/binary/{binary_name}/'\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Initialize an empty list to store the extracted text\n",
    "    text_list = []\n",
    "\n",
    "    # Check if the request was successful (status code 200)\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content of the page\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Find all the <ul> elements\n",
    "        ul_elements = soup.find_all('ul')\n",
    "\n",
    "        # Loop through the <ul> elements to find <li> and <a> elements\n",
    "        for ul in ul_elements:\n",
    "            # Find all the <li> elements within the current <ul>\n",
    "            li_elements = ul.find_all('li')\n",
    "            \n",
    "            # Loop through the <li> elements to find <a> elements\n",
    "            for li in li_elements:\n",
    "                # Find all the <a> elements within the current <li>\n",
    "                a_elements = li.find_all('a')\n",
    "                \n",
    "                # Loop through the <a> elements to extract and append their text to the list\n",
    "                for a in a_elements:\n",
    "                    text = a.get_text()\n",
    "                    text_list.append(text)\n",
    "    binary_pkg_v_map={}\n",
    "    for text in text_list:\n",
    "        # Define a regular expression pattern to match 'XXX' and 'ZZZ@WWW'\n",
    "        # pattern = r'/(\\b[^()]+\\b)\\s\\(source:\\s*[^()]+\\s([^()]+)\\)/gm'\n",
    "        pattern = r'(\\S+)\\s\\(source:\\s*[^()]+\\s(\\S+)\\)'\n",
    "\n",
    "        # Use re.search to find the pattern in the text\n",
    "        match = re.search(pattern, text)\n",
    "\n",
    "        # Check if a match is found\n",
    "        if match:\n",
    "            binary_v = match.group(1)  \n",
    "            pkg_v = match.group(2)  \n",
    "            binary_pkg_v_map[binary_v]=pkg_v\n",
    "\n",
    "    df1 = pd.DataFrame(binary_pkg_v_map.items(), columns=['binary_v', 'source_v'])\n",
    "    return df1\n",
    "\n",
    "def get_binary_source(binary_name):\n",
    "    '''\n",
    "\n",
    "    '''\n",
    "    # Send an HTTP GET request to the URL\n",
    "    url = f'https://tracker.debian.org/pkg/{binary_name}'\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Check if the request was successful (status code 200)\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content of the page\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Find the <span> element with the specific class\n",
    "        span_element = soup.find('span', class_='list-item-key')\n",
    "\n",
    "        # Check if the <span> element was found\n",
    "        if span_element:\n",
    "            # Find the following <a> element\n",
    "            a_element = span_element.find_next('a')\n",
    "            \n",
    "            # Extract and print the text within the <a> element\n",
    "            if a_element:\n",
    "                href_text = a_element.get_text()\n",
    "            else:\n",
    "                print(\"No <a> element found following the <span>.\")\n",
    "        else:\n",
    "            print(\"No <span> element with the specified class found.\")\n",
    "    return href_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### general flow (incremental daily latest version crawl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general flow (incremental daily latest version crawl)\n",
    "\n",
    "# create connections to dbs\n",
    "conn_cve=cve_create_conn()\n",
    "conn_udd=udd_create_conn()\n",
    "conn_sqlite=sqlite_create_conn('database.db')\n",
    "\n",
    "# get all debian binary names (from scantist_library) # example: [(<binary-name>,<description>),...]\n",
    "# debian_binary_name_list=querycve_all_debian_binary(conn_cve)\n",
    "debian_binary_name_list = [('cmake',''),('qtbase5-dev','')]\n",
    "\n",
    "# get all debian binary versions (from scantist_library_version) # example: {(<binary-name>,<description>):[(<version>,]}\n",
    "binary_version_mapping = querycve_binary_version_date_mapping(conn_cve,debian_binary_name_list)\n",
    "\n",
    "# # get missing versions for each binary # example: {<binary-name>:{<version1>,<version2>,...},...}\n",
    "debian_binaries_to_crawl = queryudd_cvetriage_missing_binary_versions(conn_udd,binary_version_mapping)\n",
    "\n",
    "ftp_list=web_parse_ftp_binary_pool_latest_versions('http://ftp.debian.org/debian/pool/main/c/cmake/')\n",
    "ftp_dict=web_parse_ftp_binary_pool_latest_versions_map(ftp_list)\n",
    "\n",
    "querysqlite_insert_library_multiple(conn_sqlite,debian_binaries_to_crawl,ftp_dict)\n",
    "querysqlite_insert_library_versions_ea_binary(conn_sqlite,'cmake',debian_binaries_to_crawl,ftp_dict)\n",
    "\n",
    "conn_cve.close()\n",
    "conn_udd.close()\n",
    "conn_sqlite.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### general flow (full missing version crawl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general flow (full missing version crawl)\n",
    "\n",
    "# create connections to dbs\n",
    "conn_cve=cve_create_conn()\n",
    "conn_udd=udd_create_conn()\n",
    "conn_sqlite=sqlite_create_conn('database.db')\n",
    "\n",
    "# get all debian binary names (from scantist_library) # example: [(<binary-name>,<description>),...]\n",
    "# debian_binary_name_list=querycve_all_debian_binary(conn_cve)\n",
    "# debian_binary_name_list = [('cmake',''),('qtbase5-dev','')]\n",
    "debian_binary_name_list = [('cmake','')]\n",
    "\n",
    "# get all debian binary versions (from scantist_library_version) # example: {(<binary-name>,<description>):[(<version>,]}\n",
    "binary_version_mapping = querycve_binary_version_date_mapping(conn_cve,debian_binary_name_list)\n",
    "\n",
    "for binary_info in binary_version_mapping.keys():\n",
    "    print(f'processing {binary_info[0]}')\n",
    "    df1=web_binary_all_versions_df(binary_info[0])\n",
    "    diff= set(df1.binary_v.to_list()) - set([_[0] for _ in binary_version_mapping[binary_info]])\n",
    "    source_name=get_binary_source(binary_info[0])\n",
    "    print(f'source for {binary_info[0]} is {source_name}')\n",
    "    df2=queryudd_source_all_version_date_df(conn_udd,source_name)\n",
    "    df2['date'] = df2['date'].dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    merge_df=df1.merge(df2,left_on='source_v',right_on='version',how='outer',indicator=True)\n",
    "    merge_df[merge_df['_merge']=='both'][['binary_v','date']].to_records(index=False)\n",
    "    new_versions_to_add = list(merge_df[merge_df['_merge']=='both'][['binary_v','date']].to_records(index=False))\n",
    "    querysqlite_insert_library_versions_ea_binary2(conn_sqlite,binary_info[0],new_versions_to_add)\n",
    "    versions_no_date=merge_df[merge_df['_merge']=='left_only'].binary_v.to_list()\n",
    "    print(f'Added {len(new_versions_to_add)}')\n",
    "    print(f'Could not obtain date for the following versions [{len(versions_no_date)}]:')\n",
    "    for _ in merge_df[merge_df['_merge']=='left_only'].binary_v.to_list():\n",
    "        print(_)\n",
    "\n",
    "conn_cve.close()\n",
    "conn_udd.close()\n",
    "conn_sqlite.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Archive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Debian binaries from CVETRIAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import psycopg2\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv('../pipeline_monitoring/.env')\n",
    "\n",
    "# Replace these with your own database credentials\n",
    "host2 = os.environ.get('CVETRIAGE_POSTGRES_HOSTNAME')\n",
    "database2 = os.environ.get('CVETRIAGE_POSTGRES_DB')\n",
    "user2 = os.environ.get('CVETRIAGE_POSTGRES_USERNAME')\n",
    "password2 = os.environ.get('CVETRIAGE_POSTGRES_PASSWORD')\n",
    "\n",
    "# Establish the connection\n",
    "connection_cve = psycopg2.connect(\n",
    "    host=host2,\n",
    "    database=database2,\n",
    "    user=user2,\n",
    "    password=password2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_debian_select_binaries = f'''SELECT name\n",
    "from scantist_library\n",
    "where platform='Debian'\n",
    "and is_valid=True\n",
    "'''\n",
    "df_debian_binaries = pd.read_sql_query(query_debian_select_binaries, connection_cve)\n",
    "debian_binary_list = df_debian_binaries.name.to_list()\n",
    "# connection_cve.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for testing purpose\n",
    "debian_binary_list = ['debhelper','qtbase5-dev','qtbase5-private-dev','deepin-gettext-tools','libdtkwidget-dev','libdtkcore-dev','libdtkgui-dev','libdtkcore5-bin','pkg-config','qttools5-dev-tools','cmake']\n",
    "\n",
    "binary_version_mapping = {}\n",
    "cursor = connection_cve.cursor()\n",
    "for binary_name in debian_binary_list:\n",
    "    # Execute your SQL query\n",
    "    query_binary_versions = f'''SELECT slv.version_number, slv.release_date from scantist_library sl\n",
    "    join scantist_library_version slv on slv.library_id=sl.id\n",
    "    where sl.name='{binary_name}'\n",
    "    and sl.platform='Debian'\n",
    "    and slv.is_valid=True\n",
    "    order by slv.release_date desc\n",
    "    '''\n",
    "    cursor.execute(query_binary_versions)\n",
    "\n",
    "    # Fetch all the rows as a list of tuples\n",
    "    result = cursor.fetchall()\n",
    "    binary_version_mapping[binary_name] = result\n",
    "cursor.close()\n",
    "# connection_cve.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## read from UDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def udd_create_conn():\n",
    "    host1 = \"udd-mirror.debian.net\"\n",
    "    database1 = \"udd\"\n",
    "    user1 = \"udd-mirror\"\n",
    "    password1 = \"udd-mirror\"\n",
    "\n",
    "    # Establish the connection\n",
    "    connection_udd = psycopg2.connect(\n",
    "        host=host1,\n",
    "        database=database1,\n",
    "        user=user1,\n",
    "        password=password1\n",
    "    )\n",
    "    return connection_udd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor = connection_udd.cursor()\n",
    "debian_binaries_to_crawl = {}\n",
    "for binary_name in binary_version_mapping.keys():\n",
    "    # Execute your SQL query\n",
    "    query_latest_binary_versions = f\"SELECT distinct version FROM public.packages where package='{binary_name}'\"\n",
    "    cursor.execute(query_latest_binary_versions)\n",
    "    # Fetch all the rows as a list of tuples\n",
    "    udd_latest_versions_tup_list = cursor.fetchall()\n",
    "    udd_latest_versions_set = set([tup[0] for tup in udd_latest_versions_tup_list])\n",
    "    cve_versions_set = set(tup[0] for tup in binary_version_mapping[binary_name])\n",
    "    missing_latest_versions_set = udd_latest_versions_set - cve_versions_set\n",
    "    debian_binaries_to_crawl[binary_name]=missing_latest_versions_set\n",
    "cursor.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## webcrawl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def parse_ftp_binary_pool_latest_versions(url):\n",
    "    file_info_list = []\n",
    "    # Send an HTTP GET request to the URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Check if the request was successful (status code 200)\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content of the webpage using BeautifulSoup\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Find all <tr> elements on the webpage\n",
    "        tr_elements = soup.find_all('tr')\n",
    "\n",
    "        # Extract the names and last modified timestamps from each <tr> element\n",
    "        \n",
    "        for tr in tr_elements:\n",
    "            td_elements = tr.find_all('td')\n",
    "            if len(td_elements) >= 4:\n",
    "                name = td_elements[1].text.strip()\n",
    "                last_modified = td_elements[2].text.strip()\n",
    "                file_info_list.append((name, last_modified))\n",
    "        file_info_list.remove(('Parent Directory', ''))\n",
    "    else:\n",
    "        print(f\"Failed to retrieve the webpage. Status code: {response.status_code}\")\n",
    "    \n",
    "    return file_info_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ftp_list=parse_ftp_binary_pool_latest_versions('http://ftp.debian.org/debian/pool/main/c/cmake/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ftp_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ftp_dict={}\n",
    "for _ in ftp_list:\n",
    "    if _[0].endswith('.deb'): # filter out non deb files\n",
    "        binary_name=_[0].split('_')[0]\n",
    "        binary_version=_[0].split('_')[1]\n",
    "        \n",
    "        release_date=_[1].split(' ')[0]\n",
    "        if binary_name not in ftp_dict:\n",
    "            ftp_dict[binary_name]={}\n",
    "        if binary_version in ftp_dict[binary_name]:\n",
    "            ftp_dict[binary_name][binary_version].update([release_date])\n",
    "        else:\n",
    "            ftp_dict[binary_name][binary_version]=set([release_date])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## temp db for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "# Create or connect to an SQLite database file (e.g., \"mydatabase.db\")\n",
    "conn = sqlite3.connect(\"database.db\")\n",
    "\n",
    "# Create a cursor object\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Create a table\n",
    "cursor.execute('''CREATE TABLE IF NOT EXISTS scantist_library (\n",
    "\tid INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "\tcreated timestamptz NOT NULL,\n",
    "\tmodified timestamptz NOT NULL,\n",
    "\t\"name\" varchar(255) NULL,\n",
    "\tdescription text NULL,\n",
    "\tvendor varchar(255) NULL,\n",
    "\t\"language\" varchar(64) NULL,\n",
    "\tplatform varchar(255) NULL,\n",
    "\t\"source\" varchar(255) NULL,\n",
    "\tprocessed_time timestamptz NULL,\n",
    "\tis_valid bool NULL,\n",
    "\tCONSTRAINT scantist_library_name_vendor_platform_source_db9dacce_uniq UNIQUE (name, vendor, platform)\n",
    ");''')\n",
    "\n",
    "cursor.execute('''CREATE TABLE IF NOT EXISTS scantist_library_version (\n",
    "\tid INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "\tcreated timestamptz NOT NULL,\n",
    "\tmodified timestamptz NOT NULL,\n",
    "\tversion_number varchar(255) NULL,\n",
    "\trelease_date timestamptz NULL,\n",
    "\tis_active bool NOT NULL,\n",
    "\tis_officially_supported bool NOT NULL,\n",
    "\tlibrary_id int4 NULL,\n",
    "\tlicense_id int4 NULL,\n",
    "\tis_clean bool NOT NULL,\n",
    "\t\"source\" varchar(16) NULL,\n",
    "\tprocessed_time timestamptz NULL,\n",
    "\tis_valid bool NULL,\n",
    "\tCONSTRAINT scantist_library_version_unique_key UNIQUE (library_id, version_number)\n",
    ");''')\n",
    "\n",
    "# Commit the changes to save them to the database file\n",
    "conn.commit()\n",
    "\n",
    "# Close the cursor and the connection\n",
    "cursor.close()\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def insert_latest_version(binary_name):\n",
    "    debian_binary_tup=[]\n",
    "    curr_time=datetime.now().strftime(\"%Y-%m-%d\")\n",
    "    for v in debian_binaries_to_crawl[binary_name]:\n",
    "        if v in ftp_dict[binary_name].keys():\n",
    "            debian_binary_tup.append((curr_time,curr_time,binary_name,'','','','Debian','Scantist',None,True))\n",
    "        \n",
    "    columns_library=', '.join(['created','modified','name','description','vendor','language','platform','source','processed_time','is_valid'])\n",
    "    columns_library_version=', '.join(['created','modified','version_number','release_date','is_active','is_officially_supported','library_id','license_id','is_clean','source','processed_time','is_valid'])\n",
    "    # Insert the values into the table using executemany\n",
    "    sql_query_insert_library = f\"INSERT OR IGNORE INTO scantist_library ({columns_library}) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\"\n",
    "    sql_query_insert_library_version = f\"INSERT OR IGNORE INTO scantist_library_version ({columns_library_version}) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\"\n",
    "    sql_query_select_library_id=f\"select id from scantist_library where name='{binary_name}'\"\n",
    "    # Create or connect to an SQLite database file (e.g., \"mydatabase.db\")\n",
    "\n",
    "    conn = sqlite3.connect(\"database.db\")\n",
    "    # Create a cursor object\n",
    "    cursor = conn.cursor()\n",
    "    cursor.executemany(sql_query_insert_library, debian_binary_tup)\n",
    "    # Commit the changes to save them to the database file\n",
    "    conn.commit()\n",
    "\n",
    "    cursor.execute(sql_query_select_library_id)\n",
    "    result_id = cursor.fetchall()[0][0]\n",
    "    print(f'library_id for {binary_name} is {result_id}')\n",
    "\n",
    "    debian_binary_v_tup=[]\n",
    "    for v in debian_binaries_to_crawl[binary_name]:\n",
    "        if v in ftp_dict[binary_name].keys():\n",
    "            debian_binary_v_tup.append((curr_time,curr_time,v,min(ftp_dict[binary_name][v]),True,True,result_id,None,True,'Scantist',None,True))\n",
    "\n",
    "    cursor.executemany(sql_query_insert_library_version, debian_binary_v_tup)\n",
    "    # Commit the changes to save them to the database file\n",
    "    conn.commit()\n",
    "\n",
    "    # Close the cursor and the connection\n",
    "    cursor.close()\n",
    "    conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insert_latest_version('cmake')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "def binary_all_versions_df(binary_name):\n",
    "    url = f'https://snapshot.debian.org/binary/{binary_name}/'\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Initialize an empty list to store the extracted text\n",
    "    text_list = []\n",
    "\n",
    "    # Check if the request was successful (status code 200)\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content of the page\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Find all the <ul> elements\n",
    "        ul_elements = soup.find_all('ul')\n",
    "\n",
    "        # Loop through the <ul> elements to find <li> and <a> elements\n",
    "        for ul in ul_elements:\n",
    "            # Find all the <li> elements within the current <ul>\n",
    "            li_elements = ul.find_all('li')\n",
    "            \n",
    "            # Loop through the <li> elements to find <a> elements\n",
    "            for li in li_elements:\n",
    "                # Find all the <a> elements within the current <li>\n",
    "                a_elements = li.find_all('a')\n",
    "                \n",
    "                # Loop through the <a> elements to extract and append their text to the list\n",
    "                for a in a_elements:\n",
    "                    text = a.get_text()\n",
    "                    text_list.append(text)\n",
    "    binary_pkg_v_map={}\n",
    "    for text in text_list:\n",
    "        # Define a regular expression pattern to match 'XXX' and 'ZZZ@WWW'\n",
    "        # pattern = r'/(\\b[^()]+\\b)\\s\\(source:\\s*[^()]+\\s([^()]+)\\)/gm'\n",
    "        pattern = r'(\\S+)\\s\\(source:\\s*[^()]+\\s(\\S+)\\)'\n",
    "\n",
    "        # Use re.search to find the pattern in the text\n",
    "        match = re.search(pattern, text)\n",
    "\n",
    "        # Check if a match is found\n",
    "        if match:\n",
    "            binary_v = match.group(1)  \n",
    "            pkg_v = match.group(2)  \n",
    "            binary_pkg_v_map[binary_v]=pkg_v\n",
    "\n",
    "    df1 = pd.DataFrame(binary_pkg_v_map.items(), columns=['binary_v', 'package_v'])\n",
    "    return df1\n",
    "\n",
    "def udd_source_all_version_date(source_name):\n",
    "    connection_udd= udd_create_conn()\n",
    "    query = f\"SELECT version, date FROM upload_history where source='{source_name}'\"\n",
    "    df = pd.read_sql_query(query, connection_udd)\n",
    "    connection_udd.close()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_A=binary_all_versions_df('cmake')\n",
    "df_B=udd_source_all_version_date('cmake')\n",
    "merged_df = df_A.merge(df_B, left_on='package_v', right_on='version', how='outer', indicator=True)\n",
    "version_date_map = list(merged_df[merged_df['_merge']=='both'][['version','date']].to_records(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"SELECT version, date FROM upload_history where source='cmake'\"\n",
    "df2 = pd.read_sql_query(query, connection_udd)\n",
    "# connection_udd.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = df1.merge(df2, left_on='package_v', right_on='version', how='outer', indicator=True)  # Use 'how' parameter to specify the type of join (inner, outer, left, right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df[merged_df['_merge']=='both']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## verify with cvetriage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df2[merged_df2['_merge']=='right_only']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
